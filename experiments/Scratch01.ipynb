{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "qvcJ1Y2kcEtw"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import zipfile\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1623,
     "status": "ok",
     "timestamp": 1741957986658,
     "user": {
      "displayName": "Victor Miguel Chura Quispe",
      "userId": "09733756909964094114"
     },
     "user_tz": -60
    },
    "id": "mqw26vA_Rf-Y",
    "outputId": "5c964276-3193-448d-b221-6bd8c66f0d30"
   },
   "source": [
    "drive.mount('/content/drive/')\n",
    "checkpoint_path = \"/content/drive/MyDrive/model_checkpoint_scratch_01.pth\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4ulkPtwZfJG7"
   },
   "source": "data_dir = '/content/drive/My Drive/data/classification/dataset_CIFAR10.zip'",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23097,
     "status": "ok",
     "timestamp": 1741958013303,
     "user": {
      "displayName": "Victor Miguel Chura Quispe",
      "userId": "09733756909964094114"
     },
     "user_tz": -60
    },
    "id": "Xz-CVh_iNtUB",
    "outputId": "e559805e-25b3-4235-b8e7-e1ad588248eb"
   },
   "source": [
    "!mkdir /content/dataset/\n",
    "!unzip \"/content/drive/My Drive/data/classification/dataset_CIFAR10.zip\" -d \"/content/dataset/\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0LdFF7g5hUKM"
   },
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=\"/content/dataset/train\", transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=\"/content/dataset/validation\", transform=test_transform)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1741958067697,
     "user": {
      "displayName": "Victor Miguel Chura Quispe",
      "userId": "09733756909964094114"
     },
     "user_tz": -60
    },
    "id": "YNk4nPGsfDoM",
    "outputId": "ea59c299-2214-4ed9-8dd0-51e47d35d0d3"
   },
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "      return torch.softmax(self.forward(x), dim=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(train_dataset.classes)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lDS3O3etYKrI"
   },
   "source": [
    "def train_iteration(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    average_loss = 0\n",
    "    average_accuracy = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        predictions = model(images)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        average_loss += loss.item()\n",
    "        average_accuracy += (predictions.argmax(1) == labels).type(torch.float).sum().item()\n",
    "\n",
    "    average_loss /= len(dataloader)\n",
    "    average_accuracy /= size\n",
    "    return {'average_loss': average_loss, 'average_accuracy': average_accuracy}\n",
    "\n",
    "def test_iteration(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    average_loss = 0\n",
    "    average_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            predictions = model(images)\n",
    "            average_loss += loss_fn(predictions, labels).item()\n",
    "            average_accuracy += (predictions.argmax(1) == labels).type(torch.float).sum().item()\n",
    "    average_loss /= num_batches\n",
    "    average_accuracy /= size\n",
    "    return {'average_loss': average_loss, 'average_accuracy': average_accuracy}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1741958076380,
     "user": {
      "displayName": "Victor Miguel Chura Quispe",
      "userId": "09733756909964094114"
     },
     "user_tz": -60
    },
    "id": "YuZGFzeVN1Sz",
    "outputId": "3797d89c-083a-48c6-fd16-0506afbe7035"
   },
   "source": [
    "model = ImageClassifier(num_classes=len(train_dataset.classes)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "try:\n",
    "  checkpoint = torch.load(checkpoint_path)\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  start_epoch = checkpoint['epoch']\n",
    "  print(f\"Resuming training from epoch {start_epoch}\")\n",
    "except FileNotFoundError:\n",
    "  print(\"No checkpoint found. Starting training from scratch.\")\n",
    "  start_epoch = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7Bsri_WRe0WJ"
   },
   "source": [
    "import json\n",
    "\n",
    "def save_metrics(epoch, train_results, test_results, filename=\"/content/drive/MyDrive/training_metrics_scratch_01.json\"):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        metrics = []\n",
    "\n",
    "    metrics.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_results['average_loss'],\n",
    "        'train_accuracy': train_results['average_accuracy'],\n",
    "        'test_loss': test_results['average_loss'],\n",
    "        'test_accuracy': test_results['average_accuracy']\n",
    "    })\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTeT6lVUTJNx",
    "outputId": "6d219341-3b64-4935-d571-72ee47b917f9"
   },
   "source": [
    "num_epochs = 30\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  train_results = train_iteration(train_loader, model, criterion, optimizer)\n",
    "  test_results = test_iteration(test_loader, model, criterion)\n",
    "  save_metrics(epoch, train_results, test_results)\n",
    "  print(\"Metrics saved to training_metrics_scratch_01.json\")\n",
    "  print(f\"Train Loss: {train_results['average_loss']}, Train Accuracy: {train_results['average_accuracy']}\")\n",
    "  print(f\"Test Loss: {test_results['average_loss']}, Test Accuracy: {test_results['average_accuracy']}\")\n",
    "  checkpoint = {\n",
    "      'epoch': epoch + 1,\n",
    "      'model_state_dict': model.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'train_loss': train_results['average_loss'],\n",
    "      'train_accuracy': train_results['average_accuracy'],\n",
    "      'test_loss': test_results['average_loss'],\n",
    "      'test_accuracy': test_results['average_accuracy']\n",
    "  }\n",
    "  torch.save(checkpoint, checkpoint_path)\n",
    "  print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "print(\"Finished Training\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "mount_file_id": "14C4dfm4lemojyE4gf9PjeQHUs2id6K4r",
   "authorship_tag": "ABX9TyOy/efRyfhrYoSbb5m2Ahcu"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
