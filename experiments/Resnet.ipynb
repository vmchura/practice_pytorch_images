{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "71ojRN6RWRGP"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import zipfile\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch.optim import lr_scheduler"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "checkpoint_path = \"/content/drive/My Drive/resnet_01.pth\"\n",
    "metrics_path = \"/content/drive/My Drive/training_metrics_resnet_01.json\"\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MViQVJJtWlmz",
    "outputId": "4fa487e2-84f1-4808-fee7-c22a74152c96"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "data_dir = '/content/drive/My Drive/data/classification/dataset_CIFAR10.zip'\n",
    "!mkdir /content/dataset/\n",
    "!unzip \"/content/drive/My Drive/data/classification/dataset_CIFAR10.zip\" -d \"/content/dataset/\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nNtmSMK-XBy9",
    "outputId": "fade6bd7-a86a-4d14-f796-5919c88e4347"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=\"/content/dataset/train\", transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=\"/content/dataset/validation\", transform=test_transform)\n"
   ],
   "metadata": {
    "id": "p7SdlDymXIl4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(train_dataset.classes)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1BMVgdpXTDe",
    "outputId": "c5afe362-95bc-4465-8766-66ff9f073c9d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def train_iteration(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Train Epoch: {}, Loss: {:.6f}'.format(i, loss.item()))\n",
    "\n",
    "    return None\n",
    "\n",
    "def test_iteration(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    average_loss = 0\n",
    "    average_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            predictions = model(images)\n",
    "            average_loss += loss_fn(predictions, labels).item()\n",
    "            average_accuracy += (predictions.argmax(1) == labels).type(torch.float).sum().item()\n",
    "    average_loss /= num_batches\n",
    "    average_accuracy /= size\n",
    "    return {'average_loss': average_loss, 'average_accuracy': average_accuracy}"
   ],
   "metadata": {
    "id": "lC8bNaEQXUtm"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The initial 30 epochs will run with only feature extractor, only the 3 residual blocks and the classifier will update"
  },
  {
   "cell_type": "code",
   "source": [
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, len(train_dataset.classes))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer_ft.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    exp_lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")\n",
    "    start_epoch = 0"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXSAD_1CXW6Z",
    "outputId": "6e42d4a8-6868-45df-eb36-4f2635076520"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After the initial 30 epochs, manually return here and unfreeze all layers"
  },
  {
   "cell_type": "code",
   "source": [
    "if start_epoch >= 30:\n",
    "  for param in model.parameters():\n",
    "      param.requires_grad = True\n",
    "  print('Unfreezing all layers')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFhddP5IDDhX",
    "outputId": "72231063-0071-40c9-9fbb-214409004469"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def save_metrics(epoch, train_results, test_results, filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        metrics = []\n",
    "\n",
    "    metrics.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_results['average_loss'],\n",
    "        'train_accuracy': train_results['average_accuracy'],\n",
    "        'test_loss': test_results['average_loss'],\n",
    "        'test_accuracy': test_results['average_accuracy']\n",
    "    })\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)"
   ],
   "metadata": {
    "id": "UKZe_rpbXZCT"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "num_epochs = 60\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    _ = train_iteration(train_loader, model, criterion, optimizer_ft)\n",
    "    train_results = test_iteration(train_loader, model, criterion)\n",
    "    test_results = test_iteration(test_loader, model, criterion)\n",
    "    save_metrics(epoch, train_results, test_results, metrics_path)\n",
    "    print(f\"Metrics saved to {metrics_path}\")\n",
    "    print(f\"Train Loss: {train_results['average_loss']}, Train Accuracy: {train_results['average_accuracy']}\")\n",
    "    print(f\"Test Loss: {test_results['average_loss']}, Test Accuracy: {test_results['average_accuracy']}\")\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_ft.state_dict(),\n",
    "        'scheduler_state_dict': exp_lr_scheduler.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "print(\"Finished Training\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbjYhrfBXawH",
    "outputId": "36ba02a2-4195-48cf-939a-4f500a6c4b82"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "a3jL4mxeXimx"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
